# Crowdfunding-ETL

## Summary

>During this analysis we needed to extract, transform, and load the data. In addition to prepping the data we generated new reports that are clean and easy to read. However, in order to accomplish this goal we use the following tools: PostgreSQL, Python, and Jupyter notebook.

>Task preformed:
1.	With one Large Excel files we created four individual separate CSV files by extracting and transforming the data.
2.	By utilizing the quick data based diagrams, ERD, and PostgreSQL database we created tables with many different factors to display the data.
3.	 Loading the CSV files into database.
4.	Lastly, with the loaded CSV files we were able to generate multiples reports for the stakeholders.


The screenshot below demonstrate one of the many task we were able to succeed/clean during this process.

![eamil_contact_remaining_goal_amount](https://user-images.githubusercontent.com/114452770/204672369-b7d731c2-7b72-4d6a-be1d-85aba8659fa0.PNG)
